{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ece87e2b",
   "metadata": {},
   "source": [
    "# Advanced Analytics for Brazilian E-Commerce Dataset\n",
    "\n",
    "This notebook focuses on advanced analytics, including customer churn prediction, sales forecasting, and recommendation systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e80bbd",
   "metadata": {},
   "source": [
    "## 1. Customer Churn Prediction\n",
    "\n",
    "We aim to predict customer churn using features such as recency, frequency, monetary value, and delivery performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21979e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a54b385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Import our data loading module\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from src.data_processing.data_loader import OlistDataLoader\n",
    "from config.settings import RAW_DATA_DIR, DATE_COLUMNS, FEATURE_GROUPS\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Initialize data loader and load datasets\n",
    "try:\n",
    "    data_loader = OlistDataLoader(RAW_DATA_DIR)\n",
    "    datasets = data_loader.load_all_datasets()\n",
    "    \n",
    "    # Get preprocessed data\n",
    "    processed_data = data_loader.get_preprocessed_data()\n",
    "    orders = processed_data['orders']\n",
    "    customer_features = processed_data['customer_features']\n",
    "    \n",
    "    print(\"Data loaded and preprocessed successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading data: {e}\")\n",
    "    print(\"Please ensure all data files are in the data/raw/ directory\")\n",
    "\n",
    "# Display the first few rows of the features\n",
    "print(\"\\nCustomer features for churn prediction:\")\n",
    "display(customer_features.head())\n",
    "\n",
    "# Check class balance\n",
    "churn_distribution = customer_features['churned'].value_counts(normalize=True) * 100\n",
    "print(f\"\\nChurn distribution: {churn_distribution[1]:.2f}% churned, {churn_distribution[0]:.2f}% active\")\n",
    "\n",
    "# Prepare features and target\n",
    "X = customer_features.drop(['customer_id', 'churned', 'first_purchase_date', 'last_purchase_date'], axis=1)\n",
    "y = customer_features['churned']\n",
    "\n",
    "# Split categorical and numerical features\n",
    "categorical_features = ['customer_state']\n",
    "numerical_features = [col for col in X.columns if col not in categorical_features]\n",
    "\n",
    "# Create preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='median')),\n",
    "            ('scaler', StandardScaler())\n",
    "        ]), numerical_features),\n",
    "        ('cat', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "            ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "        ]), categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Create and train the model\n",
    "model = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', GradientBoostingClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"\\nModel Evaluation:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71831bff",
   "metadata": {},
   "source": [
    "## 2. Sales Forecasting\n",
    "\n",
    "We use time series models to forecast sales by category and region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6b0f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries for time series forecasting\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "# Import additional libraries for time series forecasting\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from pmdarima import auto_arima\n",
    "import warnings\n",
    "\n",
    "# Import time series settings from config\n",
    "from config.settings import (\n",
    "    TIME_WINDOWS,\n",
    "    FORECAST_HORIZONS,\n",
    "    FORECAST_HORIZON,\n",
    "    SEASONAL_PERIODS,\n",
    "    FIGURE_SIZES,\n",
    "    COLORS\n",
    ")\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Use proper figure size and colors from config\n",
    "plt.style.use(PLOT_STYLE)\n",
    "default_figsize = FIGURE_SIZES['medium']\n",
    "\n",
    "# Sales Forecasting Functions\n",
    "def prepare_sales_data(category=None, state=None):\n",
    "    \"\"\"\n",
    "    Prepare sales data for forecasting by category and state.\n",
    "    \n",
    "    Parameters:\n",
    "        category (str): Product category to forecast (default: most frequent)\n",
    "        state (str): Customer state to forecast (default: most frequent)\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (weekly_sales, category, state) - The prepared time series data and metadata\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Use data from centralized loader\n",
    "        datasets = data_loader.get_preprocessed_data()\n",
    "        orders = datasets['orders']\n",
    "        products = datasets['products']\n",
    "        \n",
    "        sales_data = orders.merge(\n",
    "            order_items[['order_id', 'price']], \n",
    "            on='order_id'\n",
    "        ).merge(\n",
    "            products[['product_id', 'product_category_name']], \n",
    "            on='product_id'\n",
    "        ).merge(\n",
    "            customers[['customer_id', 'customer_state']], \n",
    "            on='customer_id'\n",
    "        )\n",
    "        \n",
    "        # Select category and state if not provided\n",
    "        if category is None:\n",
    "            category = sales_data['product_category_name'].value_counts().index[0]\n",
    "        if state is None:\n",
    "            state = sales_data['customer_state'].value_counts().index[0]\n",
    "            \n",
    "        print(f\"Preparing data for category '{category}' in state '{state}'\")\n",
    "        \n",
    "        # Filter and aggregate data\n",
    "        filtered_sales = sales_data[\n",
    "            (sales_data['product_category_name'] == category) & \n",
    "            (sales_data['customer_state'] == state)\n",
    "        ]\n",
    "        \n",
    "        daily_sales = filtered_sales.groupby(\n",
    "            sales_data['order_purchase_timestamp'].dt.date\n",
    "        )['price'].sum().reset_index()\n",
    "        daily_sales['order_purchase_timestamp'] = pd.to_datetime(daily_sales['order_purchase_timestamp'])\n",
    "        \n",
    "        # Create complete time series\n",
    "        date_range = pd.date_range(\n",
    "            start=sales_data['order_purchase_timestamp'].min(),\n",
    "            end=sales_data['order_purchase_timestamp'].max()\n",
    "        )\n",
    "        ts_data = pd.DataFrame({'date': date_range})\n",
    "        ts_data = ts_data.merge(daily_sales, left_on='date', right_on='order_purchase_timestamp', how='left')\n",
    "        ts_data['price'] = ts_data['price'].fillna(0)\n",
    "        ts_data.set_index('date', inplace=True)\n",
    "        \n",
    "        # Resample to weekly data\n",
    "        weekly_sales = ts_data['price'].resample('W').sum()\n",
    "        \n",
    "        return weekly_sales, category, state\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error preparing sales data: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "def train_forecast_model(weekly_sales):\n",
    "    \"\"\"\n",
    "    Train a SARIMA model for sales forecasting.\n",
    "    \n",
    "    Parameters:\n",
    "        weekly_sales (pd.Series): Weekly sales time series data\n",
    "    \n",
    "    Returns:\n",
    "        object: Fitted SARIMA model\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Find best SARIMA parameters\n",
    "        auto_model = auto_arima(\n",
    "            weekly_sales,\n",
    "            seasonal=True,\n",
    "            m=52,  # Weekly seasonality\n",
    "            start_p=0, start_q=0,\n",
    "            max_p=3, max_q=3,\n",
    "            d=None, max_d=2,\n",
    "            D=None, max_D=1,\n",
    "            trace=True,\n",
    "            error_action='ignore',\n",
    "            suppress_warnings=True,\n",
    "            stepwise=True\n",
    "        )\n",
    "        \n",
    "        print(f\"Best ARIMA model: {auto_model.order}, seasonal: {auto_model.seasonal_order}\")\n",
    "        \n",
    "        # Fit final model\n",
    "        model = SARIMAX(\n",
    "            weekly_sales,\n",
    "            order=auto_model.order,\n",
    "            seasonal_order=auto_model.seasonal_order,\n",
    "            enforce_stationarity=False,\n",
    "            enforce_invertibility=False\n",
    "        )\n",
    "        \n",
    "        return model.fit(disp=False)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error training forecast model: {e}\")\n",
    "        return None\n",
    "\n",
    "def evaluate_forecast_model(model, test_data, category, state):\n",
    "    \"\"\"\n",
    "    Evaluate the performance of the sales forecasting model.\n",
    "    \n",
    "    Parameters:\n",
    "        model: Fitted SARIMA model\n",
    "        test_data (pd.Series): Test data to evaluate against\n",
    "        category (str): Product category name\n",
    "        state (str): Customer state\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Make predictions\n",
    "        predictions = model.get_forecast(steps=len(test_data))\n",
    "        forecast = predictions.predicted_mean\n",
    "        conf_int = predictions.conf_int()\n",
    "        \n",
    "        # Calculate error metrics\n",
    "        mse = ((test_data - forecast) ** 2).mean()\n",
    "        rmse = np.sqrt(mse)\n",
    "        mape = np.abs((test_data - forecast) / test_data).mean() * 100\n",
    "        \n",
    "        print(f\"\\nForecast Evaluation for {category} in {state}:\")\n",
    "        print(f\"Root Mean Square Error: {rmse:.2f}\")\n",
    "        print(f\"Mean Absolute Percentage Error: {mape:.2f}%\")\n",
    "        \n",
    "        # Plot actual vs predicted\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(test_data.index, test_data, label='Actual')\n",
    "        plt.plot(test_data.index, forecast, label='Forecast')\n",
    "        plt.fill_between(test_data.index,\n",
    "                        conf_int.iloc[:, 0],\n",
    "                        conf_int.iloc[:, 1],\n",
    "                        color='gray', alpha=0.2)\n",
    "        plt.title(f'Sales Forecast Evaluation - {category} in {state}')\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Sales (BRL)')\n",
    "        plt.legend()\n",
    "        plt.grid(True, linestyle='--', alpha=0.7)\n",
    "        plt.show()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error evaluating forecast model: {e}\")\n",
    "\n",
    "def run_sales_forecast(category=None, state=None, test_size=0.2):\n",
    "    \"\"\"\n",
    "    Run the complete sales forecasting pipeline.\n",
    "    \n",
    "    Parameters:\n",
    "        category (str): Product category to forecast\n",
    "        state (str): Customer state to forecast\n",
    "        test_size (float): Proportion of data to use for testing\n",
    "    \"\"\"\n",
    "    # Prepare data\n",
    "    weekly_sales, category, state = prepare_sales_data(category, state)\n",
    "    if weekly_sales is None:\n",
    "        return\n",
    "    \n",
    "    # Split into train and test\n",
    "    train_size = int(len(weekly_sales) * (1 - test_size))\n",
    "    train_data = weekly_sales[:train_size]\n",
    "    test_data = weekly_sales[train_size:]\n",
    "    \n",
    "    # Train model\n",
    "    model = train_forecast_model(train_data)\n",
    "    if model is None:\n",
    "        return\n",
    "        \n",
    "    # Evaluate model\n",
    "    evaluate_forecast_model(model, test_data, category, state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3dba682",
   "metadata": {},
   "source": [
    "## 3. Recommendation System\n",
    "\n",
    "We build a collaborative filtering-based recommendation system to suggest products to customers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf4c7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries for recommendation systems\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# Use data from the centralized loader for recommendation system\n",
    "datasets = data_loader.get_preprocessed_data()\n",
    "orders = datasets['orders']\n",
    "products = datasets['products']\n",
    "\n",
    "# Create user-item matrix if not already created\n",
    "if 'user_item_matrix' not in globals():\n",
    "    print(\"Creating user-item matrix...\")\n",
    "    # Create user-item matrix (customer-product interactions)\n",
    "    user_item_data = order_items.merge(orders[['order_id', 'customer_id']], on='order_id')\n",
    "    user_item_data = user_item_data.merge(products[['product_id', 'product_category_name']], on='product_id')\n",
    "    \n",
    "    # Count purchases of each product category by each customer\n",
    "    purchase_counts = user_item_data.groupby(['customer_id', 'product_category_name']).size().reset_index(name='purchase_count')\n",
    "    \n",
    "    # Create a pivot table: customers x product categories\n",
    "    user_item_matrix = purchase_counts.pivot(\n",
    "        index='customer_id',\n",
    "        columns='product_category_name',\n",
    "        values='purchase_count'\n",
    "    ).fillna(0)\n",
    "    \n",
    "    # Calculate item-item similarity matrix using cosine similarity\n",
    "    sparse_user_item = csr_matrix(user_item_matrix.values)\n",
    "    item_similarity = cosine_similarity(sparse_user_item.T)\n",
    "    item_similarity_df = pd.DataFrame(\n",
    "        item_similarity,\n",
    "        index=user_item_matrix.columns,\n",
    "        columns=user_item_matrix.columns\n",
    "    )\n",
    "else:\n",
    "    print(\"Using existing user-item matrix...\")\n",
    "\n",
    "# Function to get top N similar items\n",
    "def get_similar_categories(category_name, n=5):\n",
    "    \"\"\"\n",
    "    Find the top N most similar product categories to a given category.\n",
    "    \n",
    "    Parameters:\n",
    "        category_name (str): Name of the product category to find similarities for\n",
    "        n (int): Number of similar categories to return (default: 5)\n",
    "    \n",
    "    Returns:\n",
    "        pd.Series: Top N similar categories with their similarity scores\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if category_name not in item_similarity_df.index:\n",
    "            print(f\"Category '{category_name}' not found in the dataset\")\n",
    "            return pd.Series()\n",
    "        \n",
    "        similar_categories = item_similarity_df[category_name].sort_values(ascending=False)\n",
    "        # Exclude the category itself\n",
    "        similar_categories = similar_categories.drop(category_name, errors='ignore')\n",
    "        return similar_categories.head(n)\n",
    "    except Exception as e:\n",
    "        print(f\"Error finding similar categories: {e}\")\n",
    "        return pd.Series()\n",
    "\n",
    "# Function to recommend products for a customer\n",
    "def recommend_for_customer(customer_id, n_recommendations=5):\n",
    "    \"\"\"\n",
    "    Generate product category recommendations for a specific customer.\n",
    "    \n",
    "    Parameters:\n",
    "        customer_id (str): ID of the customer to generate recommendations for\n",
    "        n_recommendations (int): Number of recommendations to generate (default: 5)\n",
    "    \n",
    "    Returns:\n",
    "        pd.Series: Top N recommended categories with their scores\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if customer_id not in user_item_matrix.index:\n",
    "            print(f\"Customer '{customer_id}' not found in the dataset\")\n",
    "            return pd.Series()\n",
    "        \n",
    "        # Get the customer's purchase history\n",
    "        customer_purchases = user_item_matrix.loc[customer_id]\n",
    "        \n",
    "        # Initialize recommendation scores\n",
    "        recommendation_scores = pd.Series(0, index=user_item_matrix.columns)\n",
    "        \n",
    "        # For each category the customer has purchased\n",
    "        for category, count in customer_purchases.items():\n",
    "            if count > 0:\n",
    "                # Get similar categories\n",
    "                similar_categories = item_similarity_df[category]\n",
    "                # Weight by purchase count\n",
    "                recommendation_scores += similar_categories * count\n",
    "        \n",
    "        # Remove categories the customer has already purchased\n",
    "        purchased_categories = customer_purchases[customer_purchases > 0].index\n",
    "        recommendation_scores = recommendation_scores.drop(purchased_categories, errors='ignore')\n",
    "        \n",
    "        return recommendation_scores.sort_values(ascending=False).head(n_recommendations)\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating recommendations: {e}\")\n",
    "        return pd.Series()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
