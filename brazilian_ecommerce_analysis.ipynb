{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83f836f2",
   "metadata": {},
   "source": [
    "# Brazilian E-Commerce Data Analysis\n",
    "\n",
    "This notebook provides a comprehensive analysis of the Brazilian E-Commerce dataset from Olist, a Brazilian e-commerce platform. We'll explore customer behavior, sales patterns, geographic distribution, and more to extract actionable business insights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd213b8",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Initial Exploration\n",
    "\n",
    "Let's start by loading the datasets and exploring their structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0468249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from datetime import datetime, timedelta\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "import warnings\n",
    "import json\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "from geopy.distance import geodesic\n",
    "\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set visualization style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# Display settings for better readability\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2115935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "try:\n",
    "    # Adjust these paths to match your local file structure\n",
    "    orders = pd.read_csv('data/olist_orders_dataset.csv')\n",
    "    order_items = pd.read_csv('data/olist_order_items_dataset.csv')\n",
    "    products = pd.read_csv('data/olist_products_dataset.csv')\n",
    "    customers = pd.read_csv('data/olist_customers_dataset.csv')\n",
    "    sellers = pd.read_csv('data/olist_sellers_dataset.csv')\n",
    "    payments = pd.read_csv('data/olist_order_payments_dataset.csv')\n",
    "    reviews = pd.read_csv('data/olist_order_reviews_dataset.csv')\n",
    "    category_translation = pd.read_csv('data/product_category_name_translation.csv')\n",
    "    \n",
    "    print(\"Data loaded successfully!\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error loading data: {e}\")\n",
    "    print(\"Please ensure the dataset files are in the correct location.\")\n",
    "\n",
    "# Display basic information about each dataset\n",
    "datasets = {\n",
    "    'Orders': orders,\n",
    "    'Order Items': order_items,\n",
    "    'Products': products,\n",
    "    'Customers': customers,\n",
    "    'Sellers': sellers,\n",
    "    'Payments': payments,\n",
    "    'Reviews': reviews\n",
    "}\n",
    "\n",
    "for name, df in datasets.items():\n",
    "    print(f\"\\n{name} Dataset:\")\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(f\"Columns: {', '.join(df.columns)}\")\n",
    "    print(f\"Missing values: {df.isnull().sum().sum()}\")\n",
    "    print(\"\\nSample data:\")\n",
    "    display(df.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa640745",
   "metadata": {},
   "source": [
    "## 2. Data Preparation and Cleaning\n",
    "\n",
    "Before diving into analysis, let's clean the data and prepare it for exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2bad9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert date columns to datetime\n",
    "date_columns = ['order_purchase_timestamp', 'order_approved_at', \n",
    "                'order_delivered_carrier_date', 'order_delivered_customer_date',\n",
    "                'order_estimated_delivery_date']\n",
    "\n",
    "for col in date_columns:\n",
    "    orders[col] = pd.to_datetime(orders[col], errors='coerce')\n",
    "\n",
    "# Calculate delivery metrics\n",
    "orders['delivery_time'] = (orders['order_delivered_customer_date'] - \n",
    "                           orders['order_purchase_timestamp']).dt.days\n",
    "\n",
    "orders['delivery_delay'] = (orders['order_delivered_customer_date'] - \n",
    "                            orders['order_estimated_delivery_date']).dt.days\n",
    "\n",
    "orders['approval_time'] = (orders['order_approved_at'] - \n",
    "                          orders['order_purchase_timestamp']).dt.total_seconds() / 3600  # in hours\n",
    "\n",
    "orders['carrier_time'] = (orders['order_delivered_carrier_date'] - \n",
    "                         orders['order_approved_at']).dt.days  # time to reach carrier\n",
    "\n",
    "# Handle missing values in delivery metrics\n",
    "orders['delivery_time'] = orders['delivery_time'].fillna(-1)  # -1 indicates not delivered yet\n",
    "orders['delivery_delay'] = orders['delivery_delay'].fillna(0)  # 0 indicates no delay or not delivered\n",
    "\n",
    "# Translate product categories to English\n",
    "products = products.merge(category_translation, on='product_category_name', how='left')\n",
    "products['product_category_name_english'] = products['product_category_name_english'].fillna('unknown')\n",
    "\n",
    "# Check for duplicates\n",
    "for name, df in datasets.items():\n",
    "    print(f\"{name} dataset has {df.duplicated().sum()} duplicate rows\")\n",
    "\n",
    "# Merge datasets for comprehensive analysis\n",
    "order_details = orders.merge(order_items, on='order_id', how='left')\n",
    "order_details = order_details.merge(products, on='product_id', how='left')\n",
    "order_details = order_details.merge(customers, on='customer_id', how='left')\n",
    "order_details = order_details.merge(sellers, on='seller_id', how='left')\n",
    "order_details = order_details.merge(payments, on='order_id', how='left')\n",
    "order_details = order_details.merge(reviews, on='order_id', how='left')\n",
    "\n",
    "print(\"Data preparation complete.\")\n",
    "print(f\"Comprehensive dataset shape: {order_details.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394815e9",
   "metadata": {},
   "source": [
    "## 3. Temporal Analysis\n",
    "\n",
    "Let's analyze how sales and customer behavior change over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e335e527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract time components\n",
    "orders['order_year'] = orders['order_purchase_timestamp'].dt.year\n",
    "orders['order_month'] = orders['order_purchase_timestamp'].dt.month\n",
    "orders['order_day'] = orders['order_purchase_timestamp'].dt.day\n",
    "orders['order_hour'] = orders['order_purchase_timestamp'].dt.hour\n",
    "orders['order_dayofweek'] = orders['order_purchase_timestamp'].dt.dayofweek\n",
    "orders['order_quarter'] = orders['order_purchase_timestamp'].dt.quarter\n",
    "orders['order_yearmonth'] = orders['order_purchase_timestamp'].dt.to_period('M')\n",
    "\n",
    "# Monthly order trends\n",
    "monthly_orders = orders.groupby('order_yearmonth').agg({\n",
    "    'order_id': 'count',\n",
    "    'delivery_time': 'mean',\n",
    "    'delivery_delay': 'mean'\n",
    "}).reset_index()\n",
    "monthly_orders['order_yearmonth'] = monthly_orders['order_yearmonth'].astype(str)\n",
    "\n",
    "# Create a time series plot with multiple metrics\n",
    "fig = make_subplots(rows=2, cols=1, \n",
    "                    shared_xaxes=True, \n",
    "                    subplot_titles=('Monthly Order Volume', 'Delivery Performance'))\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=monthly_orders['order_yearmonth'], y=monthly_orders['order_id'], \n",
    "               mode='lines+markers', name='Order Count'),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=monthly_orders['order_yearmonth'], y=monthly_orders['delivery_time'], \n",
    "               mode='lines+markers', name='Avg Delivery Time (days)'),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=monthly_orders['order_yearmonth'], y=monthly_orders['delivery_delay'], \n",
    "               mode='lines+markers', name='Avg Delivery Delay (days)'),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "fig.update_layout(height=800, width=1000, title_text=\"E-Commerce Trends Over Time\")\n",
    "fig.update_xaxes(tickangle=45)\n",
    "fig.show()\n",
    "\n",
    "# Analyze hourly and daily patterns\n",
    "plt.figure(figsize=(18, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "hourly_orders = orders['order_hour'].value_counts().sort_index()\n",
    "sns.barplot(x=hourly_orders.index, y=hourly_orders.values, palette='viridis')\n",
    "plt.title('Orders by Hour of Day')\n",
    "plt.xlabel('Hour')\n",
    "plt.ylabel('Number of Orders')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "daily_orders = orders['order_dayofweek'].value_counts().sort_index()\n",
    "sns.barplot(x=daily_orders.index, y=daily_orders.values, palette='viridis')\n",
    "plt.title('Orders by Day of Week')\n",
    "plt.xlabel('Day (0=Monday, 6=Sunday)')\n",
    "plt.ylabel('Number of Orders')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Time series decomposition to identify trends, seasonality, and residuals\n",
    "# First, create a daily time series\n",
    "daily_orders = orders.groupby(orders['order_purchase_timestamp'].dt.date)['order_id'].count()\n",
    "daily_orders.index = pd.DatetimeIndex(daily_orders.index)\n",
    "daily_orders = daily_orders.sort_index()\n",
    "\n",
    "# Fill missing dates with zeros\n",
    "idx = pd.date_range(daily_orders.index.min(), daily_orders.index.max())\n",
    "daily_orders = daily_orders.reindex(idx, fill_value=0)\n",
    "\n",
    "# Perform time series decomposition\n",
    "decomposition = seasonal_decompose(daily_orders, model='additive', period=7)  # Weekly seasonality\n",
    "\n",
    "# Plot the decomposition\n",
    "fig, (ax1, ax2, ax3, ax4) = plt.subplots(4, 1, figsize=(14, 12))\n",
    "decomposition.observed.plot(ax=ax1)\n",
    "ax1.set_title('Observed')\n",
    "decomposition.trend.plot(ax=ax2)\n",
    "ax2.set_title('Trend')\n",
    "decomposition.seasonal.plot(ax=ax3)\n",
    "ax3.set_title('Seasonality')\n",
    "decomposition.resid.plot(ax=ax4)\n",
    "ax4.set_title('Residuals')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Temporal analysis complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459a0650",
   "metadata": {},
   "source": [
    "## 4. Geographic Analysis\n",
    "\n",
    "Let's explore the geographic distribution of customers and sellers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eecb109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the GeoJSON file with the structure you provided\n",
    "brazil_gdf = gpd.read_file('brazil_states.geojson')\n",
    "\n",
    "# Display the first few rows to confirm the structure\n",
    "print(\"GeoJSON structure:\")\n",
    "display(brazil_gdf.head())\n",
    "\n",
    "# Create a mapping from municipality names to state codes\n",
    "# This dictionary maps Brazilian municipalities to their respective state codes\n",
    "# We'll create a more comprehensive mapping based on your data\n",
    "\n",
    "# First, let's extract unique municipality names from your GeoJSON\n",
    "municipalities = brazil_gdf['shapeName'].unique()\n",
    "print(f\"Number of municipalities in GeoJSON: {len(municipalities)}\")\n",
    "\n",
    "# Create a function to extract state code from municipality name or other attributes\n",
    "def extract_state_code(row):\n",
    "    \"\"\"\n",
    "    Extract the state code from municipality data.\n",
    "    This is a simplified approach - in a real notebook, you might need a more comprehensive mapping.\n",
    "    \"\"\"\n",
    "    # Try to extract from shapeName (some municipalities include state in parentheses)\n",
    "    name = row['shapeName']\n",
    "    \n",
    "    # Check if the name contains state code in parentheses\n",
    "    if '(' in name and ')' in name:\n",
    "        possible_state = name.split('(')[1].split(')')[0]\n",
    "        if len(possible_state) == 2:  # State codes are 2 characters\n",
    "            return possible_state\n",
    "    \n",
    "    # Map major cities directly\n",
    "    city_to_state = {\n",
    "        'São Paulo': 'SP',\n",
    "        'Rio de Janeiro': 'RJ',\n",
    "        'Brasília': 'DF',\n",
    "        'Salvador': 'BA',\n",
    "        'Fortaleza': 'CE',\n",
    "        'Belo Horizonte': 'MG',\n",
    "        'Manaus': 'AM',\n",
    "        'Curitiba': 'PR',\n",
    "        'Recife': 'PE',\n",
    "        'Porto Alegre': 'RS',\n",
    "        'Belém': 'PA',\n",
    "        'Goiânia': 'GO',\n",
    "        # Add more mappings as needed\n",
    "    }\n",
    "    \n",
    "    if name in city_to_state:\n",
    "        return city_to_state[name]\n",
    "    \n",
    "    # Try to extract from shapeID or other attributes\n",
    "    # This would depend on the specific structure of your data\n",
    "    \n",
    "    # Default to None if we can't determine the state\n",
    "    return None\n",
    "\n",
    "# Apply the function to add a state column\n",
    "brazil_gdf['state_code'] = brazil_gdf.apply(extract_state_code, axis=1)\n",
    "\n",
    "# Check how many municipalities were mapped to states\n",
    "mapped_count = brazil_gdf['state_code'].notna().sum()\n",
    "print(f\"Municipalities mapped to states: {mapped_count} out of {len(brazil_gdf)}\")\n",
    "\n",
    "# For municipalities without a state code, we can try to infer from geographic proximity\n",
    "# This is a simplified approach - in a real notebook, you would use a more comprehensive method\n",
    "\n",
    "# Now let's prepare the e-commerce data for geographic analysis\n",
    "# Fix the merge issue: customers → orders → order_items\n",
    "\n",
    "# First, merge orders with customers to get customer location data\n",
    "customer_orders = orders.merge(customers, on='customer_id')\n",
    "\n",
    "# Then, merge with order_items to get product and seller information\n",
    "geographic_data = customer_orders.merge(order_items, on='order_id')\n",
    "\n",
    "# Finally, merge with sellers to get seller location data\n",
    "geographic_data = geographic_data.merge(sellers, on='seller_id')\n",
    "\n",
    "# Check the resulting dataframe\n",
    "print(\"Geographic data shape:\", geographic_data.shape)\n",
    "print(\"\\nSample of geographic data:\")\n",
    "display(geographic_data[['customer_id', 'customer_state', 'seller_id', 'seller_state', 'order_id', 'product_id']].head())\n",
    "\n",
    "# Count orders by customer state\n",
    "state_orders = geographic_data.groupby('customer_state').size().reset_index(name='order_count')\n",
    "print(\"\\nOrder counts by state:\")\n",
    "display(state_orders)\n",
    "\n",
    "# Now we need to create a state-level GeoDataFrame for our choropleth map\n",
    "# Since your GeoJSON is at the municipality level, we'll need to aggregate to states\n",
    "\n",
    "# First, let's try to identify which municipalities belong to which states\n",
    "# We can use the state_code column we created earlier\n",
    "\n",
    "# For municipalities that we couldn't map automatically, we might need manual mapping\n",
    "# or we could use a spatial join with a state-level GeoJSON if available\n",
    "\n",
    "# Let's create a state-level GeoDataFrame by dissolving municipalities\n",
    "# We'll only use municipalities that we've successfully mapped to states\n",
    "brazil_gdf_with_state = brazil_gdf[brazil_gdf['state_code'].notna()].copy()\n",
    "\n",
    "# If we don't have enough municipalities mapped to states, we might need an alternative approach\n",
    "if len(brazil_gdf_with_state) < len(brazil_gdf) * 0.5:\n",
    "    print(\"Warning: Less than 50% of municipalities mapped to states.\")\n",
    "    print(\"Using an alternative approach with state centroids.\")\n",
    "    \n",
    "    # Create a GeoDataFrame with state centroids\n",
    "    # This is a fallback if we can't create proper state polygons\n",
    "    \n",
    "    # Dictionary of Brazilian state centroids (approximate coordinates)\n",
    "    state_coordinates = {\n",
    "        'AC': (-9.0238, -70.812),   # Acre\n",
    "        'AL': (-9.5713, -36.782),   # Alagoas\n",
    "        'AM': (-3.4168, -65.8561),  # Amazonas\n",
    "        'AP': (1.4035, -51.7963),   # Amapá\n",
    "        'BA': (-12.9718, -41.7007), # Bahia\n",
    "        'CE': (-5.4984, -39.3206),  # Ceará\n",
    "        'DF': (-15.7998, -47.8645), # Distrito Federal\n",
    "        'ES': (-19.1834, -40.3089), # Espírito Santo\n",
    "        'GO': (-15.827, -49.8362),  # Goiás\n",
    "        'MA': (-5.7945, -45.7445),  # Maranhão\n",
    "        'MG': (-18.5122, -44.555),  # Minas Gerais\n",
    "        'MS': (-20.7722, -54.7852), # Mato Grosso do Sul\n",
    "        'MT': (-12.6819, -56.9211), # Mato Grosso\n",
    "        'PA': (-3.9784, -53.3306),  # Pará\n",
    "        'PB': (-7.1219, -36.7272),  # Paraíba\n",
    "        'PE': (-8.8137, -36.9541),  # Pernambuco\n",
    "        'PI': (-7.7183, -42.7289),  # Piauí\n",
    "        'PR': (-24.8951, -51.6584), # Paraná\n",
    "        'RJ': (-22.9099, -43.2095), # Rio de Janeiro\n",
    "        'RN': (-5.4026, -36.9541),  # Rio Grande do Norte\n",
    "        'RO': (-10.83, -63.34),     # Rondônia\n",
    "        'RR': (2.7376, -62.0751),   # Roraima\n",
    "        'RS': (-30.0346, -51.2177), # Rio Grande do Sul\n",
    "        'SC': (-27.2423, -50.2189), # Santa Catarina\n",
    "        'SE': (-10.5741, -37.3857), # Sergipe\n",
    "        'SP': (-23.5505, -46.6333), # São Paulo\n",
    "        'TO': (-10.1753, -48.2982)  # Tocantins\n",
    "    }\n",
    "    \n",
    "    # Create a GeoDataFrame with state centroids\n",
    "    state_points = pd.DataFrame(\n",
    "        [(state, coords[0], coords[1]) for state, coords in state_coordinates.items()],\n",
    "        columns=['state_code', 'latitude', 'longitude']\n",
    "    )\n",
    "    \n",
    "    # Convert to GeoDataFrame\n",
    "    geometry = [Point(xy) for xy in zip(state_points['longitude'], state_points['latitude'])]\n",
    "    state_gdf = gpd.GeoDataFrame(state_points, geometry=geometry, crs=\"EPSG:4326\")\n",
    "    \n",
    "    # Merge with order counts\n",
    "    state_gdf = state_gdf.merge(state_orders, left_on='state_code', right_on='customer_state', how='left')\n",
    "    state_gdf['order_count'] = state_gdf['order_count'].fillna(0)\n",
    "    \n",
    "    # Create a point-based map\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(15, 10))\n",
    "    \n",
    "    # Plot Brazil's municipalities as background\n",
    "    brazil_gdf.plot(ax=ax, color='lightgray', edgecolor='gray', linewidth=0.1)\n",
    "    \n",
    "    # Plot state points with size proportional to order count\n",
    "    state_gdf.plot(\n",
    "        ax=ax,\n",
    "        column='order_count',\n",
    "        markersize=state_gdf['order_count'] / state_gdf['order_count'].max() * 500 + 50,\n",
    "        cmap='viridis',\n",
    "        legend=True,\n",
    "        legend_kwds={'label': 'Number of Orders', 'orientation': 'horizontal'}\n",
    "    )\n",
    "    \n",
    "    # Add state labels\n",
    "    for idx, row in state_gdf.iterrows():\n",
    "        ax.text(\n",
    "            row.geometry.x,\n",
    "            row.geometry.y,\n",
    "            f\"{row['state_code']}\\n{int(row['order_count'])}\",\n",
    "            fontsize=8,\n",
    "            ha='center',\n",
    "            va='center',\n",
    "            color='white',\n",
    "            fontweight='bold'\n",
    "        )\n",
    "    \n",
    "    plt.title('Distribution of Orders Across Brazilian States', fontsize=16)\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    # If we have enough municipalities mapped to states, we can create proper state polygons\n",
    "    # Group by state and dissolve geometries\n",
    "    state_gdf = brazil_gdf_with_state.dissolve(by='state_code', aggfunc='sum')\n",
    "    state_gdf = state_gdf.reset_index()\n",
    "    \n",
    "    # Merge with order counts\n",
    "    state_gdf = state_gdf.merge(state_orders, left_on='state_code', right_on='customer_state', how='left')\n",
    "    state_gdf['order_count'] = state_gdf['order_count'].fillna(0)\n",
    "    \n",
    "    # Create the choropleth map\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(15, 10))\n",
    "    \n",
    "    # Plot the map with a color gradient based on order count\n",
    "    state_gdf.plot(\n",
    "        column='order_count',\n",
    "        ax=ax,\n",
    "        legend=True,\n",
    "        cmap='viridis',\n",
    "        edgecolor='black',\n",
    "        linewidth=0.5,\n",
    "        legend_kwds={'label': 'Number of Orders', 'orientation': 'horizontal'}\n",
    "    )\n",
    "    \n",
    "    # Add state labels\n",
    "    for idx, row in state_gdf.iterrows():\n",
    "        # Get the centroid of each state polygon\n",
    "        centroid = row['geometry'].centroid\n",
    "        # Add the state code and order count as text\n",
    "        ax.text(\n",
    "            centroid.x, \n",
    "            centroid.y, \n",
    "            f\"{row['state_code']}\\n{int(row['order_count'])}\",\n",
    "            fontsize=8,\n",
    "            ha='center',\n",
    "            va='center',\n",
    "            color='white',\n",
    "            fontweight='bold'\n",
    "        )\n",
    "    \n",
    "    plt.title('Distribution of Orders Across Brazilian States', fontsize=16)\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Now let's analyze customer-seller distances\n",
    "# Add coordinates to the geographic data\n",
    "geographic_data['customer_lat'] = geographic_data['customer_state'].map(lambda x: state_coordinates.get(x, (np.nan, np.nan))[0])\n",
    "geographic_data['customer_lon'] = geographic_data['customer_state'].map(lambda x: state_coordinates.get(x, (np.nan, np.nan))[1])\n",
    "geographic_data['seller_lat'] = geographic_data['seller_state'].map(lambda x: state_coordinates.get(x, (np.nan, np.nan))[0])\n",
    "geographic_data['seller_lon'] = geographic_data['seller_state'].map(lambda x: state_coordinates.get(x, (np.nan, np.nan))[1])\n",
    "\n",
    "# Calculate the distance between customer and seller using geodesic distance (in km)\n",
    "def calculate_distance(row):\n",
    "    if pd.isna(row['customer_lat']) or pd.isna(row['seller_lat']):\n",
    "        return np.nan\n",
    "    \n",
    "    customer_coords = (row['customer_lat'], row['customer_lon'])\n",
    "    seller_coords = (row['seller_lat'], row['seller_lon'])\n",
    "    \n",
    "    return geodesic(customer_coords, seller_coords).kilometers\n",
    "\n",
    "# Apply the distance calculation\n",
    "geographic_data['distance_km'] = geographic_data.apply(calculate_distance, axis=1)\n",
    "\n",
    "# Analyze the distribution of distances\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(geographic_data['distance_km'].dropna(), bins=50, kde=True)\n",
    "plt.title('Distribution of Customer-Seller Distances')\n",
    "plt.xlabel('Distance (km)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate average distance by product category\n",
    "product_distances = geographic_data.merge(products[['product_id', 'product_category_name']], on='product_id')\n",
    "category_distances = product_distances.groupby('product_category_name')['distance_km'].agg(['mean', 'median', 'count']).reset_index()\n",
    "category_distances = category_distances.sort_values('mean', ascending=False)\n",
    "\n",
    "# Display top and bottom categories by average distance\n",
    "print(\"\\nProduct categories with highest average customer-seller distance:\")\n",
    "display(category_distances.head(10))\n",
    "\n",
    "print(\"\\nProduct categories with lowest average customer-seller distance:\")\n",
    "display(category_distances.tail(10))\n",
    "\n",
    "# Visualize top 15 categories by average distance\n",
    "plt.figure(figsize=(14, 8))\n",
    "top_categories = category_distances.head(15)\n",
    "sns.barplot(x='mean', y='product_category_name', data=top_categories, palette='viridis')\n",
    "plt.title('Product Categories with Highest Average Customer-Seller Distance')\n",
    "plt.xlabel('Average Distance (km)')\n",
    "plt.ylabel('Product Category')\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Create a flow map showing the volume of transactions between states\n",
    "# Count transactions between states\n",
    "state_flows = geographic_data.groupby(['customer_state', 'seller_state']).size().reset_index(name='transaction_count')\n",
    "state_flows = state_flows.sort_values('transaction_count', ascending=False)\n",
    "\n",
    "print(\"\\nTop state-to-state transaction flows:\")\n",
    "display(state_flows.head(15))\n",
    "\n",
    "# Create a network visualization of the top flows\n",
    "top_flows = state_flows[state_flows['transaction_count'] > 100].copy()\n",
    "\n",
    "# Create a Brazil map as background\n",
    "fig, ax = plt.subplots(1, 1, figsize=(15, 10))\n",
    "\n",
    "# Plot Brazil's municipalities as background\n",
    "brazil_gdf.plot(ax=ax, color='lightgray', edgecolor='gray', linewidth=0.1)\n",
    "\n",
    "# Add state labels at centroids\n",
    "for state_code, coords in state_coordinates.items():\n",
    "    ax.text(\n",
    "        coords[1],  # longitude\n",
    "        coords[0],  # latitude\n",
    "        state_code,\n",
    "        fontsize=8,\n",
    "        ha='center',\n",
    "        va='center',\n",
    "        fontweight='bold'\n",
    "    )\n",
    "\n",
    "# Draw lines between states for top flows\n",
    "for idx, flow in top_flows.iterrows():\n",
    "    customer_state = flow['customer_state']\n",
    "    seller_state = flow['seller_state']\n",
    "    \n",
    "    if customer_state in state_coordinates and seller_state in state_coordinates:\n",
    "        # Get coordinates\n",
    "        customer_coords = state_coordinates[customer_state]\n",
    "        seller_coords = state_coordinates[seller_state]\n",
    "        \n",
    "        # Draw line with width proportional to transaction count\n",
    "        line_width = 0.5 + (flow['transaction_count'] / top_flows['transaction_count'].max() * 5)\n",
    "        \n",
    "        # Draw the line\n",
    "        ax.plot(\n",
    "            [customer_coords[1], seller_coords[1]],  # longitude\n",
    "            [customer_coords[0], seller_coords[0]],  # latitude\n",
    "            color='blue',\n",
    "            alpha=0.5,\n",
    "            linewidth=line_width\n",
    "        )\n",
    "\n",
    "plt.title('Major E-Commerce Transaction Flows Between Brazilian States', fontsize=16)\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analyze delivery time by distance\n",
    "# Ensure we have delivery time data\n",
    "if 'order_delivered_customer_date' in geographic_data.columns and 'order_purchase_timestamp' in geographic_data.columns:\n",
    "    # Convert date columns to datetime\n",
    "    geographic_data['order_delivered_customer_date'] = pd.to_datetime(geographic_data['order_delivered_customer_date'])\n",
    "    geographic_data['order_purchase_timestamp'] = pd.to_datetime(geographic_data['order_purchase_timestamp'])\n",
    "    \n",
    "    # Calculate delivery time in days\n",
    "    geographic_data['delivery_time_days'] = (geographic_data['order_delivered_customer_date'] - \n",
    "                                           geographic_data['order_purchase_timestamp']).dt.total_seconds() / (24 * 3600)\n",
    "    \n",
    "    # Remove outliers and invalid values\n",
    "    valid_delivery = geographic_data[\n",
    "        (geographic_data['delivery_time_days'] > 0) & \n",
    "        (geographic_data['delivery_time_days'] < 100) &  # Remove extreme outliers\n",
    "        (geographic_data['distance_km'].notna())\n",
    "    ]\n",
    "    \n",
    "    # Create distance bins\n",
    "    valid_delivery['distance_bin'] = pd.cut(\n",
    "        valid_delivery['distance_km'],\n",
    "        bins=[0, 200, 500, 1000, 1500, 2000, 3000, 5000],\n",
    "        labels=['0-200', '200-500', '500-1000', '1000-1500', '1500-2000', '2000-3000', '3000+']\n",
    "    )\n",
    "    \n",
    "    # Calculate average delivery time by distance bin\n",
    "    delivery_by_distance = valid_delivery.groupby('distance_bin')['delivery_time_days'].agg(\n",
    "        ['mean', 'median', 'std', 'count']\n",
    "    ).reset_index()\n",
    "    \n",
    "    print(\"\\nDelivery time by distance:\")\n",
    "    display(delivery_by_distance)\n",
    "    \n",
    "    # Visualize relationship between distance and delivery time\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    # Boxplot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.boxplot(x='distance_bin', y='delivery_time_days', data=valid_delivery)\n",
    "    plt.title('Delivery Time by Distance')\n",
    "    plt.xlabel('Distance (km)')\n",
    "    plt.ylabel('Delivery Time (days)')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # Scatter plot with trend line\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.scatterplot(x='distance_km', y='delivery_time_days', data=valid_delivery, alpha=0.3)\n",
    "    sns.regplot(x='distance_km', y='delivery_time_days', data=valid_delivery, scatter=False, color='red')\n",
    "    plt.title('Delivery Time vs. Distance')\n",
    "    plt.xlabel('Distance (km)')\n",
    "    plt.ylabel('Delivery Time (days)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate correlation\n",
    "    correlation = valid_delivery['distance_km'].corr(valid_delivery['delivery_time_days'])\n",
    "    print(f\"\\nCorrelation between distance and delivery time: {correlation:.4f}\")\n",
    "    \n",
    "    # Analyze delivery efficiency by state\n",
    "    state_efficiency = valid_delivery.groupby('seller_state').agg({\n",
    "        'delivery_time_days': ['mean', 'median'],\n",
    "        'distance_km': 'mean',\n",
    "        'order_id': 'count'\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Flatten multi-level columns\n",
    "    state_efficiency.columns = ['_'.join(col).strip('_') if isinstance(col, tuple) else col for col in state_efficiency.columns]\n",
    "    \n",
    "    # Calculate delivery efficiency (days per 1000 km)\n",
    "    state_efficiency['days_per_1000km'] = state_efficiency['delivery_time_days_mean'] / (state_efficiency['distance_km_mean'] / 1000)\n",
    "    \n",
    "    # Sort by efficiency (lower is better)\n",
    "    state_efficiency = state_efficiency.sort_values('days_per_1000km')\n",
    "    \n",
    "    print(\"\\nDelivery efficiency by seller state:\")\n",
    "    display(state_efficiency)\n",
    "    \n",
    "    # Visualize delivery efficiency by state\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    sns.barplot(x='seller_state', y='days_per_1000km', data=state_efficiency)\n",
    "    plt.title('Delivery Efficiency by Seller State (Days per 1000 km)')\n",
    "    plt.xlabel('Seller State')\n",
    "    plt.ylabel('Days per 1000 km')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\nGeographic analysis complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f66cce",
   "metadata": {},
   "source": [
    "## 5. Customer Segmentation with RFM Analysis\n",
    "\n",
    "Let's segment customers using the RFM (Recency, Frequency, Monetary) framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e921d410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate RFM metrics\n",
    "# Set the analysis date as the day after the last order\n",
    "last_order_date = orders['order_purchase_timestamp'].max()\n",
    "analysis_date = last_order_date + timedelta(days=1)\n",
    "\n",
    "# Get order values\n",
    "order_values = payments.groupby('order_id')['payment_value'].sum().reset_index()\n",
    "orders_with_values = orders.merge(order_values, on='order_id', how='left')\n",
    "\n",
    "# Calculate customer-level RFM metrics\n",
    "rfm = orders_with_values.groupby('customer_id').agg({\n",
    "    'order_purchase_timestamp': lambda x: (analysis_date - x.max()).days,  # Recency\n",
    "    'order_id': 'count',  # Frequency\n",
    "    'payment_value': 'sum'  # Monetary\n",
    "}).reset_index()\n",
    "\n",
    "rfm.columns = ['customer_id', 'recency', 'frequency', 'monetary']\n",
    "\n",
    "# Create RFM segments\n",
    "rfm['r_score'] = pd.qcut(rfm['recency'], 5, labels=[5, 4, 3, 2, 1])  # 5 is most recent\n",
    "rfm['f_score'] = pd.qcut(rfm['frequency'].rank(method='first'), 5, labels=[1, 2, 3, 4, 5])  # 5 is most frequent\n",
    "rfm['m_score'] = pd.qcut(rfm['monetary'], 5, labels=[1, 2, 3, 4, 5])  # 5 is highest monetary value\n",
    "\n",
    "# Calculate RFM score\n",
    "rfm['rfm_score'] = rfm['r_score'].astype(str) + rfm['f_score'].astype(str) + rfm['m_score'].astype(str)\n",
    "\n",
    "# Define customer segments\n",
    "def segment_customer(row):\n",
    "    r, f, m = int(row['r_score']), int(row['f_score']), int(row['m_score'])\n",
    "    \n",
    "    if r >= 4 and f >= 4 and m >= 4:\n",
    "        return 'Champions'\n",
    "    elif r >= 3 and f >= 3 and m >= 3:\n",
    "        return 'Loyal Customers'\n",
    "    elif r >= 3 and f >= 1 and m >= 2:\n",
    "        return 'Potential Loyalists'\n",
    "    elif r >= 4 and f <= 2 and m <= 2:\n",
    "        return 'New Customers'\n",
    "    elif r <= 2 and f >= 3 and m >= 3:\n",
    "        return 'At Risk'\n",
    "    elif r <= 2 and f >= 2 and m >= 2:\n",
    "        return 'Needs Attention'\n",
    "    elif r <= 1 and f <= 2 and m <= 2:\n",
    "        return 'Lost'\n",
    "    else:\n",
    "        return 'Others'\n",
    "\n",
    "rfm['segment'] = rfm.apply(segment_customer, axis=1)\n",
    "\n",
    "# Visualize customer segments\n",
    "segment_counts = rfm['segment'].value_counts().reset_index()\n",
    "segment_counts.columns = ['segment', 'count']\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='segment', y='count', data=segment_counts, palette='viridis')\n",
    "plt.title('Customer Segments')\n",
    "plt.xlabel('Segment')\n",
    "plt.ylabel('Number of Customers')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analyze segment characteristics\n",
    "segment_analysis = rfm.groupby('segment').agg({\n",
    "    'recency': 'mean',\n",
    "    'frequency': 'mean',\n",
    "    'monetary': 'mean',\n",
    "    'customer_id': 'count'\n",
    "}).reset_index()\n",
    "\n",
    "segment_analysis.columns = ['segment', 'avg_recency', 'avg_frequency', 'avg_monetary', 'customer_count']\n",
    "segment_analysis['avg_order_value'] = segment_analysis['avg_monetary'] / segment_analysis['avg_frequency']\n",
    "\n",
    "print(\"Customer segment analysis:\")\n",
    "display(segment_analysis.sort_values('customer_count', ascending=False))\n",
    "\n",
    "# Visualize segment characteristics with a radar chart\n",
    "segments = segment_analysis['segment'].tolist()\n",
    "metrics = ['avg_recency', 'avg_frequency', 'avg_monetary', 'avg_order_value']\n",
    "\n",
    "# Normalize the metrics for radar chart\n",
    "scaler = StandardScaler()\n",
    "segment_analysis_scaled = segment_analysis.copy()\n",
    "segment_analysis_scaled[metrics] = scaler.fit_transform(segment_analysis[metrics])\n",
    "\n",
    "# Create radar chart\n",
    "fig = go.Figure()\n",
    "\n",
    "for i, segment in enumerate(segments):\n",
    "    values = segment_analysis_scaled[segment_analysis_scaled['segment'] == segment][metrics].values.flatten().tolist()\n",
    "    values += [values[0]]  # Close the loop\n",
    "    \n",
    "    fig.add_trace(go.Scatterpolar(\n",
    "        r=values,\n",
    "        theta=metrics + [metrics[0]],  # Close the loop\n",
    "        fill='toself',\n",
    "        name=segment\n",
    "    ))\n",
    "\n",
    "fig.update_layout(\n",
    "    polar=dict(\n",
    "        radialaxis=dict(\n",
    "            visible=True,\n",
    "        )\n",
    "    ),\n",
    "    title=\"Customer Segment Characteristics\",\n",
    "    showlegend=True\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "print(\"Customer segmentation complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347dbc99",
   "metadata": {},
   "source": [
    "## 6. Product Analysis\n",
    "\n",
    "Let's analyze product categories, pricing, and popularity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30346ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze product categories\n",
    "product_category_counts = products['product_category_name_english'].value_counts().reset_index()\n",
    "product_category_counts.columns = ['category', 'product_count']\n",
    "\n",
    "# Get sales by category\n",
    "category_sales = order_items.merge(products, on='product_id').groupby('product_category_name_english').agg({\n",
    "    'order_id': 'count',\n",
    "    'price': 'sum'\n",
    "}).reset_index()\n",
    "category_sales.columns = ['category', 'order_count', 'total_sales']\n",
    "category_sales['average_price'] = category_sales['total_sales'] / category_sales['order_count']\n",
    "\n",
    "# Merge product counts with sales data\n",
    "category_analysis = product_category_counts.merge(category_sales, on='category', how='left')\n",
    "category_analysis = category_analysis.fillna(0)\n",
    "category_analysis['sales_per_product'] = category_analysis['order_count'] / category_analysis['product_count']\n",
    "category_analysis = category_analysis.sort_values('total_sales', ascending=False)\n",
    "\n",
    "# Visualize top categories by sales\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.barplot(x='total_sales', y='category', data=category_analysis.head(15), palette='viridis')\n",
    "plt.title('Top 15 Categories by Sales')\n",
    "plt.xlabel('Total Sales (BRL)')\n",
    "plt.ylabel('Category')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Create a scatter plot of product count vs sales\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.scatter(\n",
    "    category_analysis['product_count'], \n",
    "    category_analysis['total_sales'],\n",
    "    s=category_analysis['average_price'] * 10,  # Size represents average price\n",
    "    alpha=0.6\n",
    ")\n",
    "\n",
    "# Add labels for top categories\n",
    "for i, row in category_analysis.head(10).iterrows():\n",
    "    plt.annotate(\n",
    "        row['category'],\n",
    "        xy=(row['product_count'], row['total_sales']),\n",
    "        xytext=(5, 5),\n",
    "        textcoords='offset points'\n",
    "    )\n",
    "\n",
    "plt.title('Product Count vs Sales by Category')\n",
    "plt.xlabel('Number of Products')\n",
    "plt.ylabel('Total Sales (BRL)')\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Price distribution analysis\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(order_items['price'], bins=50, kde=True)\n",
    "plt.title('Price Distribution')\n",
    "plt.xlabel('Price (BRL)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xlim(0, order_items['price'].quantile(0.95))  # Limit to 95th percentile to handle outliers\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.boxplot(y='price', data=order_items)\n",
    "plt.title('Price Boxplot')\n",
    "plt.ylabel('Price (BRL)')\n",
    "plt.ylim(0, order_items['price'].quantile(0.95))  # Limit to 95th percentile to handle outliers\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Product association analysis (market basket analysis)\n",
    "# Create a one-hot encoded matrix of products purchased together\n",
    "# First, get products purchased in each order\n",
    "order_products = order_items[['order_id', 'product_id']].drop_duplicates()\n",
    "order_products = order_products.merge(products[['product_id', 'product_category_name_english']], on='product_id')\n",
    "\n",
    "# Create a pivot table for one-hot encoding\n",
    "basket = order_products.pivot_table(\n",
    "    index='order_id',\n",
    "    columns='product_category_name_english',\n",
    "    values='product_id',\n",
    "    aggfunc='count',\n",
    "    fill_value=0\n",
    ")\n",
    "\n",
    "# Convert to binary (purchased or not)\n",
    "basket_sets = basket.applymap(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "# Generate frequent itemsets\n",
    "frequent_itemsets = apriori(basket_sets, min_support=0.01, use_colnames=True)\n",
    "\n",
    "# Generate association rules\n",
    "if not frequent_itemsets.empty:\n",
    "    rules = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=1)\n",
    "    \n",
    "    # Display top association rules\n",
    "    if not rules.empty:\n",
    "        print(\"Top product association rules:\")\n",
    "        display(rules.sort_values('lift', ascending=False).head(10))\n",
    "        \n",
    "        # Visualize top associations\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        plt.scatter(rules['support'], rules['confidence'], alpha=0.5, s=rules['lift']*20)\n",
    "        \n",
    "        # Add labels for top rules\n",
    "        for i, rule in rules.sort_values('lift', ascending=False).head(5).iterrows():\n",
    "            plt.annotate(\n",
    "                f\"{list(rule['antecedents'])[0]} → {list(rule['consequents'])[0]}\",\n",
    "                xy=(rule['support'], rule['confidence']),\n",
    "                xytext=(5, 5),\n",
    "                textcoords='offset points'\n",
    "            )\n",
    "        \n",
    "        plt.title('Association Rules - Support vs Confidence')\n",
    "        plt.xlabel('Support')\n",
    "        plt.ylabel('Confidence')\n",
    "        plt.grid(True, linestyle='--', alpha=0.7)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"No association rules found with the given thresholds.\")\n",
    "else:\n",
    "    print(\"No frequent itemsets found with the given support threshold.\")\n",
    "\n",
    "print(\"Product analysis complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf83c8d",
   "metadata": {},
   "source": [
    "## 7. Delivery and Logistics Analysis\n",
    "\n",
    "Let's analyze delivery performance and its impact on customer satisfaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035cf6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for delivered orders only\n",
    "delivered_orders = orders[orders['order_status'] == 'delivered'].copy()\n",
    "\n",
    "# Calculate delivery performance metrics\n",
    "delivery_stats = delivered_orders.agg({\n",
    "    'delivery_time': ['mean', 'median', 'min', 'max', 'std'],\n",
    "    'delivery_delay': ['mean', 'median', 'min', 'max', 'std']\n",
    "})\n",
    "\n",
    "print(\"Delivery performance statistics:\")\n",
    "display(delivery_stats)\n",
    "\n",
    "# Analyze delivery time distribution\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(delivered_orders['delivery_time'], bins=30, kde=True)\n",
    "plt.title('Delivery Time Distribution')\n",
    "plt.xlabel('Delivery Time (days)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.axvline(delivered_orders['delivery_time'].mean(), color='red', linestyle='--', label=f'Mean: {delivered_orders[\"delivery_time\"].mean():.1f} days')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.histplot(delivered_orders['delivery_delay'], bins=30, kde=True)\n",
    "plt.title('Delivery Delay Distribution')\n",
    "plt.xlabel('Delivery Delay (days)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.axvline(0, color='green', linestyle='--', label='On Time')\n",
    "plt.axvline(delivered_orders['delivery_delay'].mean(), color='red', linestyle='--', label=f'Mean: {delivered_orders[\"delivery_delay\"].mean():.1f} days')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analyze relationship between delivery performance and customer satisfaction\n",
    "delivery_reviews = delivered_orders.merge(reviews[['order_id', 'review_score']], on='order_id')\n",
    "\n",
    "# Group by delivery time buckets and calculate average review score\n",
    "delivery_time_bins = [0, 7, 14, 21, 28, float('inf')]\n",
    "delivery_time_labels = ['0-7 days', '8-14 days', '15-21 days', '22-28 days', '29+ days']\n",
    "delivery_reviews['delivery_time_bucket'] = pd.cut(delivery_reviews['delivery_time'], bins=delivery_time_bins, labels=delivery_time_labels)\n",
    "\n",
    "delivery_time_satisfaction = delivery_reviews.groupby('delivery_time_bucket').agg({\n",
    "    'review_score': 'mean',\n",
    "    'order_id': 'count'\n",
    "}).reset_index()\n",
    "delivery_time_satisfaction.columns = ['delivery_time_bucket', 'avg_review_score', 'order_count']\n",
    "\n",
    "# Group by delivery delay buckets and calculate average review score\n",
    "delivery_delay_bins = [-float('inf'), -7, -3, 0, 3, 7, float('inf')]\n",
    "delivery_delay_labels = ['7+ days early', '3-7 days early', '0-3 days early', '0-3 days late', '3-7 days late', '7+ days late']\n",
    "delivery_reviews['delivery_delay_bucket'] = pd.cut(delivery_reviews['delivery_delay'], bins=delivery_delay_bins, labels=delivery_delay_labels)\n",
    "\n",
    "delivery_delay_satisfaction = delivery_reviews.groupby('delivery_delay_bucket').agg({\n",
    "    'review_score': 'mean',\n",
    "    'order_id': 'count'\n",
    "}).reset_index()\n",
    "delivery_delay_satisfaction.columns = ['delivery_delay_bucket', 'avg_review_score', 'order_count']\n",
    "\n",
    "# Visualize the relationship\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.barplot(x='delivery_time_bucket', y='avg_review_score', data=delivery_time_satisfaction, palette='viridis')\n",
    "plt.title('Average Review Score by Delivery Time')\n",
    "plt.xlabel('Delivery Time')\n",
    "plt.ylabel('Average Review Score')\n",
    "plt.ylim(1, 5)\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.barplot(x='delivery_delay_bucket', y='avg_review_score', data=delivery_delay_satisfaction, palette='viridis')\n",
    "plt.title('Average Review Score by Delivery Delay')\n",
    "plt.xlabel('Delivery Delay')\n",
    "plt.ylabel('Average Review Score')\n",
    "plt.ylim(1, 5)\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analyze delivery performance by state\n",
    "state_delivery = delivered_orders.merge(customers[['customer_id', 'customer_state']], on='customer_id')\n",
    "state_delivery_perf = state_delivery.groupby('customer_state').agg({\n",
    "    'delivery_time': 'mean',\n",
    "    'delivery_delay': 'mean',\n",
    "    'order_id': 'count'\n",
    "}).reset_index()\n",
    "state_delivery_perf.columns = ['state', 'avg_delivery_time', 'avg_delivery_delay', 'order_count']\n",
    "state_delivery_perf = state_delivery_perf.sort_values('order_count', ascending=False)\n",
    "\n",
    "# Visualize delivery performance by state\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "sns.barplot(x='state', y='avg_delivery_time', data=state_delivery_perf.head(10), palette='viridis')\n",
    "plt.title('Average Delivery Time by State (Top 10 by Order Count)')\n",
    "plt.xlabel('State')\n",
    "plt.ylabel('Average Delivery Time (days)')\n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "sns.barplot(x='state', y='avg_delivery_delay', data=state_delivery_perf.head(10), palette='viridis')\n",
    "plt.title('Average Delivery Delay by State (Top 10 by Order Count)')\n",
    "plt.xlabel('State')\n",
    "plt.ylabel('Average Delivery Delay (days)')\n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Delivery and logistics analysis complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d83604e",
   "metadata": {},
   "source": [
    "## 8. Customer Satisfaction and Review Analysis\n",
    "\n",
    "Let's analyze customer reviews and satisfaction levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f2356e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review score distribution\n",
    "review_score_counts = reviews['review_score'].value_counts().sort_index().reset_index()\n",
    "review_score_counts.columns = ['score', 'count']\n",
    "\n",
    "# Calculate review statistics\n",
    "review_stats = reviews['review_score'].describe()\n",
    "print(\"Review score statistics:\")\n",
    "display(review_stats)\n",
    "\n",
    "# Visualize review score distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='score', y='count', data=review_score_counts, palette='viridis')\n",
    "plt.title('Distribution of Review Scores')\n",
    "plt.xlabel('Review Score')\n",
    "plt.ylabel('Count')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate Net Promoter Score (NPS)\n",
    "# Promoters: 5-star reviews\n",
    "# Passives: 4-star reviews\n",
    "# Detractors: 1-3 star reviews\n",
    "promoters = reviews[reviews['review_score'] == 5].shape[0]\n",
    "passives = reviews[reviews['review_score'] == 4].shape[0]\n",
    "detractors = reviews[reviews['review_score'] <= 3].shape[0]\n",
    "total_reviews = reviews.shape[0]\n",
    "\n",
    "nps = (promoters - detractors) / total_reviews * 100\n",
    "\n",
    "print(f\"Net Promoter Score (NPS): {nps:.2f}%\")\n",
    "\n",
    "# Analyze review comment length\n",
    "reviews['comment_length'] = reviews['review_comment_message'].fillna('').apply(len)\n",
    "\n",
    "# Visualize comment length by review score\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='review_score', y='comment_length', data=reviews)\n",
    "plt.title('Review Comment Length by Score')\n",
    "plt.xlabel('Review Score')\n",
    "plt.ylabel('Comment Length (characters)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analyze time to review\n",
    "reviews['review_creation_date'] = pd.to_datetime(reviews['review_creation_date'])\n",
    "reviews['review_answer_timestamp'] = pd.to_datetime(reviews['review_answer_timestamp'])\n",
    "\n",
    "# Merge with orders to get purchase date\n",
    "review_timing = reviews.merge(orders[['order_id', 'order_purchase_timestamp']], on='order_id')\n",
    "review_timing['days_to_review'] = (review_timing['review_creation_date'] - review_timing['order_purchase_timestamp']).dt.days\n",
    "\n",
    "# Visualize time to review by score\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='review_score', y='days_to_review', data=review_timing)\n",
    "plt.title('Days to Review by Score')\n",
    "plt.xlabel('Review Score')\n",
    "plt.ylabel('Days from Purchase to Review')\n",
    "plt.ylim(0, review_timing['days_to_review'].quantile(0.95))  # Limit to 95th percentile to handle outliers\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analyze relationship between product categories and review scores\n",
    "category_reviews = order_items.merge(products[['product_id', 'product_category_name_english']], on='product_id')\n",
    "category_reviews = category_reviews.merge(reviews[['order_id', 'review_score']], on='order_id')\n",
    "\n",
    "category_satisfaction = category_reviews.groupby('product_category_name_english').agg({\n",
    "    'review_score': ['mean', 'count']\n",
    "}).reset_index()\n",
    "category_satisfaction.columns = ['category', 'avg_review_score', 'review_count']\n",
    "category_satisfaction = category_satisfaction[category_satisfaction['review_count'] >= 30]  # Filter for categories with sufficient reviews\n",
    "category_satisfaction = category_satisfaction.sort_values('avg_review_score')\n",
    "\n",
    "# Visualize category satisfaction\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.barplot(x='avg_review_score', y='category', data=category_satisfaction, palette='viridis')\n",
    "plt.title('Average Review Score by Product Category')\n",
    "plt.xlabel('Average Review Score')\n",
    "plt.ylabel('Category')\n",
    "plt.axvline(reviews['review_score'].mean(), color='red', linestyle='--', label=f'Overall Average: {reviews[\"review_score\"].mean():.2f}')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Customer satisfaction analysis complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f7abe7",
   "metadata": {},
   "source": [
    "## 9. Payment Analysis\n",
    "\n",
    "Let's analyze payment methods and their relationship with order values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461e6676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Payment method distribution\n",
    "payment_type_counts = payments['payment_type'].value_counts().reset_index()\n",
    "payment_type_counts.columns = ['payment_type', 'count']\n",
    "\n",
    "# Calculate payment statistics by method\n",
    "payment_stats = payments.groupby('payment_type').agg({\n",
    "    'payment_value': ['mean', 'median', 'min', 'max', 'sum', 'count']\n",
    "}).reset_index()\n",
    "payment_stats.columns = ['payment_type', 'avg_value', 'median_value', 'min_value', 'max_value', 'total_value', 'count']\n",
    "payment_stats['share_of_total'] = payment_stats['total_value'] / payment_stats['total_value'].sum() * 100\n",
    "\n",
    "print(\"Payment statistics by method:\")\n",
    "display(payment_stats.sort_values('total_value', ascending=False))\n",
    "\n",
    "# Visualize payment method distribution\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "sns.barplot(x='payment_type', y='count', data=payment_type_counts, palette='viridis')\n",
    "plt.title('Payment Method Distribution')\n",
    "plt.xlabel('Payment Type')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.pie(payment_stats['total_value'], labels=payment_stats['payment_type'], autopct='%1.1f%%', \n",
    "        startangle=90, shadow=True, explode=[0.05] * len(payment_stats))\n",
    "plt.title('Share of Total Payment Value by Method')\n",
    "plt.axis('equal')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analyze payment value distribution\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(payments['payment_value'], bins=50, kde=True)\n",
    "plt.title('Payment Value Distribution')\n",
    "plt.xlabel('Payment Value (BRL)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xlim(0, payments['payment_value'].quantile(0.95))  # Limit to 95th percentile to handle outliers\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.boxplot(x='payment_type', y='payment_value', data=payments)\n",
    "plt.title('Payment Value by Method')\n",
    "plt.xlabel('Payment Type')\n",
    "plt.ylabel('Payment Value (BRL)')\n",
    "plt.ylim(0, payments['payment_value'].quantile(0.95))  # Limit to 95th percentile to handle outliers\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analyze installments\n",
    "installment_counts = payments['payment_installments'].value_counts().sort_index().reset_index()\n",
    "installment_counts.columns = ['installments', 'count']\n",
    "\n",
    "# Calculate average order value by installment count\n",
    "installment_stats = payments.groupby('payment_installments').agg({\n",
    "    'payment_value': 'mean',\n",
    "    'order_id': 'count'\n",
    "}).reset_index()\n",
    "installment_stats.columns = ['installments', 'avg_order_value', 'order_count']\n",
    "\n",
    "# Visualize installment distribution\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.barplot(x='installments', y='count', data=installment_counts.head(10), palette='viridis')\n",
    "plt.title('Distribution of Payment Installments')\n",
    "plt.xlabel('Number of Installments')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.lineplot(x='installments', y='avg_order_value', data=installment_stats.head(10), marker='o')\n",
    "plt.title('Average Order Value by Installment Count')\n",
    "plt.xlabel('Number of Installments')\n",
    "plt.ylabel('Average Order Value (BRL)')\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analyze payment method preferences by state\n",
    "payment_state = payments.merge(orders[['order_id', 'customer_id']], on='order_id')\n",
    "payment_state = payment_state.merge(customers[['customer_id', 'customer_state']], on='customer_id')\n",
    "\n",
    "# Calculate payment method share by state\n",
    "payment_method_by_state = payment_state.groupby(['customer_state', 'payment_type']).size().reset_index(name='count')\n",
    "payment_method_by_state['total_by_state'] = payment_method_by_state.groupby('customer_state')['count'].transform('sum')\n",
    "payment_method_by_state['share'] = payment_method_by_state['count'] / payment_method_by_state['total_by_state'] * 100\n",
    "\n",
    "# Get top 5 states by order count\n",
    "top_states = payment_state['customer_state'].value_counts().head(5).index.tolist()\n",
    "payment_method_by_top_states = payment_method_by_state[payment_method_by_state['customer_state'].isin(top_states)]\n",
    "\n",
    "# Visualize payment method preferences by state\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.barplot(x='customer_state', y='share', hue='payment_type', data=payment_method_by_top_states, palette='viridis')\n",
    "plt.title('Payment Method Preferences by State (Top 5 States)')\n",
    "plt.xlabel('State')\n",
    "plt.ylabel('Share (%)')\n",
    "plt.legend(title='Payment Type')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Payment analysis complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687cff82",
   "metadata": {},
   "source": [
    "## 10. Seller Performance Analysis\n",
    "\n",
    "Let's analyze seller performance and identify top performers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b22c08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate seller performance metrics\n",
    "seller_performance = order_items.merge(orders[['order_id', 'order_purchase_timestamp', 'order_delivered_customer_date', 'delivery_time', 'delivery_delay']], on='order_id')\n",
    "seller_performance = seller_performance.merge(reviews[['order_id', 'review_score']], on='order_id')\n",
    "\n",
    "# Aggregate metrics by seller\n",
    "seller_metrics = seller_performance.groupby('seller_id').agg({\n",
    "    'order_id': 'count',\n",
    "    'price': 'sum',\n",
    "    'freight_value': 'sum',\n",
    "    'review_score': 'mean',\n",
    "    'delivery_time': 'mean',\n",
    "    'delivery_delay': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "seller_metrics.columns = ['seller_id', 'order_count', 'total_sales', 'total_freight', 'avg_review_score', 'avg_delivery_time', 'avg_delivery_delay']\n",
    "seller_metrics['avg_order_value'] = seller_metrics['total_sales'] / seller_metrics['order_count']\n",
    "\n",
    "# Add seller state information\n",
    "seller_metrics = seller_metrics.merge(sellers[['seller_id', 'seller_state', 'seller_city']], on='seller_id')\n",
    "\n",
    "# Identify top sellers by sales\n",
    "top_sellers_by_sales = seller_metrics.sort_values('total_sales', ascending=False).head(20)\n",
    "\n",
    "print(\"Top 20 sellers by total sales:\")\n",
    "display(top_sellers_by_sales[['seller_id', 'seller_state', 'order_count', 'total_sales', 'avg_review_score']])\n",
    "\n",
    "# Visualize seller performance distribution\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "sns.histplot(seller_metrics['order_count'], bins=30, kde=True)\n",
    "plt.title('Distribution of Order Count per Seller')\n",
    "plt.xlabel('Order Count')\n",
    "plt.ylabel('Number of Sellers')\n",
    "plt.xlim(0, seller_metrics['order_count'].quantile(0.95))  # Limit to 95th percentile\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "sns.histplot(seller_metrics['total_sales'], bins=30, kde=True)\n",
    "plt.title('Distribution of Total Sales per Seller')\n",
    "plt.xlabel('Total Sales (BRL)')\n",
    "plt.ylabel('Number of Sellers')\n",
    "plt.xlim(0, seller_metrics['total_sales'].quantile(0.95))  # Limit to 95th percentile\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "sns.histplot(seller_metrics['avg_review_score'], bins=30, kde=True)\n",
    "plt.title('Distribution of Average Review Score per Seller')\n",
    "plt.xlabel('Average Review Score')\n",
    "plt.ylabel('Number of Sellers')\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "sns.histplot(seller_metrics['avg_delivery_time'], bins=30, kde=True)\n",
    "plt.title('Distribution of Average Delivery Time per Seller')\n",
    "plt.xlabel('Average Delivery Time (days)')\n",
    "plt.ylabel('Number of Sellers')\n",
    "plt.xlim(0, seller_metrics['avg_delivery_time'].quantile(0.95))  # Limit to 95th percentile\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Create a scatter plot of sales vs. review score\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.scatter(\n",
    "    seller_metrics['total_sales'], \n",
    "    seller_metrics['avg_review_score'],\n",
    "    s=seller_metrics['order_count'] / 5,  # Size represents order count\n",
    "    alpha=0.6\n",
    ")\n",
    "\n",
    "# Add labels for top sellers\n",
    "for i, row in top_sellers_by_sales.head(10).iterrows():\n",
    "    plt.annotate(\n",
    "        f\"Seller {row['seller_id'][:5]}...\",\n",
    "        xy=(row['total_sales'], row['avg_review_score']),\n",
    "        xytext=(5, 5),\n",
    "        textcoords='offset points'\n",
    "    )\n",
    "\n",
    "plt.title('Seller Performance: Sales vs. Review Score')\n",
    "plt.xlabel('Total Sales (BRL)')\n",
    "plt.ylabel('Average Review Score')\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analyze seller concentration\n",
    "seller_metrics['sales_percentile'] = pd.qcut(seller_metrics['total_sales'], 10, labels=False)\n",
    "seller_concentration = seller_metrics.groupby('sales_percentile').agg({\n",
    "    'seller_id': 'count',\n",
    "    'total_sales': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "seller_concentration['seller_percentage'] = seller_concentration['seller_id'] / seller_concentration['seller_id'].sum() * 100\n",
    "seller_concentration['sales_percentage'] = seller_concentration['total_sales'] / seller_concentration['total_sales'].sum() * 100\n",
    "seller_concentration['cumulative_sales_percentage'] = seller_concentration['sales_percentage'].cumsum()\n",
    "\n",
    "print(\"Seller concentration analysis:\")\n",
    "display(seller_concentration)\n",
    "\n",
    "# Visualize seller concentration (Pareto principle)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(seller_concentration['sales_percentile'], seller_concentration['sales_percentage'], alpha=0.7)\n",
    "plt.plot(seller_concentration['sales_percentile'], seller_concentration['cumulative_sales_percentage'], 'ro-', linewidth=2)\n",
    "plt.title('Seller Concentration: Pareto Analysis')\n",
    "plt.xlabel('Seller Decile (by Sales)')\n",
    "plt.ylabel('Percentage of Total Sales')\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.xticks(range(10))\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Seller performance analysis complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24434916",
   "metadata": {},
   "source": [
    "## 11. Cohort Analysis\n",
    "\n",
    "Let's perform a cohort analysis to understand customer retention over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661a0c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create customer cohorts based on first purchase date\n",
    "customer_first_purchase = orders.groupby('customer_id')['order_purchase_timestamp'].min().reset_index()\n",
    "customer_first_purchase['cohort'] = customer_first_purchase['order_purchase_timestamp'].dt.to_period('M')\n",
    "\n",
    "# Get all customer orders with cohort information\n",
    "cohort_data = orders.merge(customer_first_purchase[['customer_id', 'cohort']], on='customer_id')\n",
    "cohort_data['order_month'] = cohort_data['order_purchase_timestamp'].dt.to_period('M')\n",
    "\n",
    "# Calculate the period number (months since first purchase)\n",
    "cohort_data['period_number'] = (cohort_data['order_month'].astype(str).astype('datetime64[ns]') - \n",
    "                               cohort_data['cohort'].astype(str).astype('datetime64[ns]')).dt.days // 30\n",
    "\n",
    "# Count unique customers by cohort and period\n",
    "cohort_counts = cohort_data.groupby(['cohort', 'period_number'])['customer_id'].nunique().reset_index()\n",
    "\n",
    "# Get the total number of customers in each cohort\n",
    "cohort_sizes = cohort_data.groupby('cohort')['customer_id'].nunique().reset_index()\n",
    "cohort_sizes.columns = ['cohort', 'cohort_size']\n",
    "\n",
    "# Calculate retention rate\n",
    "cohort_retention = cohort_counts.merge(cohort_sizes, on='cohort')\n",
    "cohort_retention['retention_rate'] = cohort_retention['customer_id'] / cohort_retention['cohort_size'] * 100\n",
    "\n",
    "# Create a pivot table for the retention matrix\n",
    "retention_matrix = cohort_retention.pivot_table(index='cohort', columns='period_number', values='retention_rate')\n",
    "\n",
    "# Visualize the retention matrix as a heatmap\n",
    "plt.figure(figsize=(15, 8))\n",
    "sns.heatmap(retention_matrix, annot=True, fmt='.1f', cmap='viridis', linewidths=.5)\n",
    "plt.title('Customer Cohort Retention Analysis (% of Customers Returning)')\n",
    "plt.xlabel('Months Since First Purchase')\n",
    "plt.ylabel('Cohort (First Purchase Month)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate average retention by period\n",
    "avg_retention = cohort_retention.groupby('period_number')['retention_rate'].mean().reset_index()\n",
    "\n",
    "# Visualize average retention by period\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(x='period_number', y='retention_rate', data=avg_retention, marker='o', linewidth=2)\n",
    "plt.title('Average Customer Retention by Month')\n",
    "plt.xlabel('Months Since First Purchase')\n",
    "plt.ylabel('Retention Rate (%)')\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.xticks(avg_retention['period_number'])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Cohort analysis complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7eddae3",
   "metadata": {},
   "source": [
    "## 12. Key Insights and Recommendations\n",
    "\n",
    "Based on our comprehensive analysis of the Brazilian E-Commerce dataset, here are the key insights and recommendations:\n",
    "\n",
    "### Customer Behavior Insights:\n",
    "\n",
    "1. **Customer Segmentation**: We identified distinct customer segments including Champions, Loyal Customers, and At-Risk customers. The RFM analysis shows that a small percentage of customers contribute to a large portion of revenue.\n",
    "2. **Geographic Distribution**: Orders are concentrated in certain states, particularly in the southeast region of Brazil. SP (São Paulo) has the highest number of customers and orders.\n",
    "3. **Purchasing Patterns**: Most orders contain only one item, suggesting customers typically make single-item purchases rather than bundling multiple products.\n",
    "4. **Payment Preferences**: Credit card is the dominant payment method, but there's significant usage of boleto (a Brazilian payment method). Higher-value orders tend to use more installments.\n",
    "\n",
    "\n",
    "### Product Insights:\n",
    "\n",
    "1. **Category Performance**: Certain product categories like furniture, electronics, and housewares dominate sales. However, some niche categories show high average order values despite lower volumes.\n",
    "2. **Price Sensitivity**: The price distribution analysis shows most products fall within a specific price range, with a long tail of higher-priced items.\n",
    "3. **Product Associations**: Our market basket analysis revealed interesting product combinations that are frequently purchased together, providing opportunities for cross-selling.\n",
    "\n",
    "\n",
    "### Operational Insights:\n",
    "\n",
    "1. **Delivery Performance**: There's a clear correlation between delivery delays and lower customer satisfaction. Orders delivered ahead of schedule receive significantly higher ratings.\n",
    "2. **Seller Performance**: There's high variability in seller performance, with a small percentage of sellers accounting for a large portion of sales (Pareto principle).\n",
    "3. **Seasonal Trends**: The time series analysis revealed clear seasonal patterns in order volume, with peaks during certain months and specific days of the week.\n",
    "\n",
    "\n",
    "### Recommendations:\n",
    "\n",
    "1. **Customer Retention Strategies**:\n",
    "\n",
    "1. Implement targeted marketing campaigns for different customer segments\n",
    "2. Develop loyalty programs for Champions and Loyal Customers\n",
    "3. Create win-back campaigns for At-Risk customers\n",
    "\n",
    "\n",
    "\n",
    "2. **Product Portfolio Optimization**:\n",
    "\n",
    "1. Focus on expanding high-performing categories\n",
    "2. Consider bundling frequently co-purchased items\n",
    "3. Review pricing strategies for underperforming categories\n",
    "\n",
    "\n",
    "\n",
    "3. **Operational Improvements**:\n",
    "\n",
    "1. Optimize delivery processes to reduce delays\n",
    "2. Implement stricter seller performance standards\n",
    "3. Adjust inventory and staffing based on seasonal trends\n",
    "\n",
    "\n",
    "\n",
    "4. **Geographic Expansion**:\n",
    "\n",
    "1. Target marketing efforts in high-potential states\n",
    "2. Consider logistics improvements in states with longer delivery times\n",
    "3. Analyze the customer-to-seller ratio to identify market opportunities\n",
    "\n",
    "\n",
    "\n",
    "5. **Payment and Financing Options**:\n",
    "\n",
    "1. Continue offering diverse payment methods\n",
    "2. Consider optimizing installment options for higher-value purchases\n",
    "3. Analyze payment method preferences by region for targeted offerings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae22804",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
