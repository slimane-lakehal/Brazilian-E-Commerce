{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b5c7642",
   "metadata": {},
   "source": [
    "## 1. Customer Churn Prediction\n",
    "\n",
    "We aim to predict customer churn using features such as recency, frequency, monetary value, and delivery performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b3b7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Load and preprocess data\n",
    "# Import necessary libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Load the datasets (assuming they're already loaded from previous analysis)\n",
    "try:\n",
    "    # Adjust these paths to match your local file structure\n",
    "    orders = pd.read_csv('data/olist_orders_dataset.csv')\n",
    "    order_items = pd.read_csv('data/olist_order_items_dataset.csv')\n",
    "    customers = pd.read_csv('data/olist_customers_dataset.csv')\n",
    "    payments = pd.read_csv('data/olist_order_payments_dataset.csv')\n",
    "    reviews = pd.read_csv('data/olist_order_reviews_dataset.csv')\n",
    "    \n",
    "    print(\"Data loaded successfully!\")\n",
    "except NameError:\n",
    "    print(\"Using previously loaded data\")\n",
    "\n",
    "# Convert date columns to datetime\n",
    "date_columns = ['order_purchase_timestamp', 'order_approved_at', \n",
    "                'order_delivered_carrier_date', 'order_delivered_customer_date',\n",
    "                'order_estimated_delivery_date']\n",
    "\n",
    "for col in date_columns:\n",
    "    orders[col] = pd.to_datetime(orders[col], errors='coerce')\n",
    "\n",
    "# Define the analysis date (last date in the dataset + 30 days)\n",
    "last_order_date = orders['order_purchase_timestamp'].max()\n",
    "analysis_date = last_order_date + timedelta(days=30)\n",
    "print(f\"Analysis date: {analysis_date}\")\n",
    "\n",
    "# Define churn: customers who haven't made a purchase in the last 90 days\n",
    "churn_threshold = 90  # days\n",
    "\n",
    "# Create customer purchase history features\n",
    "customer_orders = orders.groupby('customer_id').agg({\n",
    "    'order_id': 'count',\n",
    "    'order_purchase_timestamp': [\n",
    "        'min',  # first purchase date\n",
    "        'max',  # last purchase date\n",
    "        lambda x: (analysis_date - x.max()).days  # recency\n",
    "    ]\n",
    "}).reset_index()\n",
    "\n",
    "# Flatten the multi-level column names\n",
    "customer_orders.columns = ['customer_id', 'order_count', 'first_purchase_date', \n",
    "                          'last_purchase_date', 'days_since_last_purchase']\n",
    "\n",
    "# Define churn label (1 = churned, 0 = active)\n",
    "customer_orders['churned'] = (customer_orders['days_since_last_purchase'] > churn_threshold).astype(int)\n",
    "\n",
    "# Calculate time between first and last purchase (in days)\n",
    "customer_orders['customer_lifetime'] = (customer_orders['last_purchase_date'] - \n",
    "                                       customer_orders['first_purchase_date']).dt.days\n",
    "\n",
    "# Add order value features\n",
    "order_values = payments.groupby('order_id')['payment_value'].sum().reset_index()\n",
    "customer_values = orders.merge(order_values, on='order_id').groupby('customer_id').agg({\n",
    "    'payment_value': ['sum', 'mean', 'std', 'min', 'max']\n",
    "}).reset_index()\n",
    "\n",
    "# Flatten the multi-level column names\n",
    "customer_values.columns = ['customer_id', 'total_spend', 'avg_order_value', \n",
    "                          'std_order_value', 'min_order_value', 'max_order_value']\n",
    "\n",
    "# Add review features\n",
    "customer_reviews = orders.merge(reviews[['order_id', 'review_score']], on='order_id').groupby('customer_id').agg({\n",
    "    'review_score': ['mean', 'min', 'count']\n",
    "}).reset_index()\n",
    "\n",
    "# Flatten the multi-level column names\n",
    "customer_reviews.columns = ['customer_id', 'avg_review_score', 'min_review_score', 'review_count']\n",
    "\n",
    "# Add delivery experience features\n",
    "orders['delivery_delay'] = (orders['order_delivered_customer_date'] - \n",
    "                           orders['order_estimated_delivery_date']).dt.days\n",
    "\n",
    "customer_delivery = orders.groupby('customer_id').agg({\n",
    "    'delivery_delay': ['mean', 'max']\n",
    "}).reset_index()\n",
    "\n",
    "# Flatten the multi-level column names\n",
    "customer_delivery.columns = ['customer_id', 'avg_delivery_delay', 'max_delivery_delay']\n",
    "\n",
    "# Merge all customer features\n",
    "customer_features = customer_orders.merge(customer_values, on='customer_id', how='left')\n",
    "customer_features = customer_features.merge(customer_reviews, on='customer_id', how='left')\n",
    "customer_features = customer_features.merge(customer_delivery, on='customer_id', how='left')\n",
    "customer_features = customer_features.merge(customers[['customer_id', 'customer_state']], on='customer_id', how='left')\n",
    "\n",
    "# Fill missing values\n",
    "customer_features['avg_review_score'] = customer_features['avg_review_score'].fillna(customer_features['avg_review_score'].mean())\n",
    "customer_features['min_review_score'] = customer_features['min_review_score'].fillna(customer_features['min_review_score'].mean())\n",
    "customer_features['review_count'] = customer_features['review_count'].fillna(0)\n",
    "customer_features['avg_delivery_delay'] = customer_features['avg_delivery_delay'].fillna(0)\n",
    "customer_features['max_delivery_delay'] = customer_features['max_delivery_delay'].fillna(0)\n",
    "customer_features['std_order_value'] = customer_features['std_order_value'].fillna(0)\n",
    "\n",
    "# Calculate average purchase frequency (for customers with more than one purchase)\n",
    "customer_features['purchase_frequency'] = np.where(\n",
    "    customer_features['order_count'] > 1,\n",
    "    customer_features['customer_lifetime'] / (customer_features['order_count'] - 1),\n",
    "    0\n",
    ")\n",
    "\n",
    "# Display the first few rows of the features\n",
    "print(\"\\nCustomer features for churn prediction:\")\n",
    "display(customer_features.head())\n",
    "\n",
    "# Check class balance\n",
    "churn_distribution = customer_features['churned'].value_counts(normalize=True) * 100\n",
    "print(f\"\\nChurn distribution: {churn_distribution[1]:.2f}% churned, {churn_distribution[0]:.2f}% active\")\n",
    "\n",
    "# Prepare features and target\n",
    "X = customer_features.drop(['customer_id', 'churned', 'first_purchase_date', 'last_purchase_date'], axis=1)\n",
    "y = customer_features['churned']\n",
    "\n",
    "# Split categorical and numerical features\n",
    "categorical_features = ['customer_state']\n",
    "numerical_features = [col for col in X.columns if col not in categorical_features]\n",
    "\n",
    "# Create preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='median')),\n",
    "            ('scaler', StandardScaler())\n",
    "        ]), numerical_features),\n",
    "        ('cat', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "            ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "        ]), categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Create and train the model\n",
    "model = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', GradientBoostingClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"\\nModel Evaluation:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
